{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** looks like the training set has 40,000 data-points and the test set has 10,000 data-points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join(\"../data/\")\n",
    "\n",
    "def load_churn_data(path=DATA_PATH, train=True):\n",
    "    if train:\n",
    "        csv_path = os.path.join(path, \"churn_train.csv\")\n",
    "    else:\n",
    "        csv_path = os.path.join(path, \"churn_test.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load train data\n",
    "churn_train_orig = load_churn_data(path=DATA_PATH, train=True)\n",
    "churn_train = churn_train_orig.copy()\n",
    "\n",
    "#load train data\n",
    "churn_test_orig = load_churn_data(path=DATA_PATH, train=False)\n",
    "churn_test = churn_test_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 12 columns):\n",
      "avg_dist                  40000 non-null float64\n",
      "avg_rating_by_driver      39838 non-null float64\n",
      "avg_rating_of_driver      33472 non-null float64\n",
      "avg_surge                 40000 non-null float64\n",
      "city                      40000 non-null object\n",
      "last_trip_date            40000 non-null object\n",
      "phone                     39681 non-null object\n",
      "signup_date               40000 non-null object\n",
      "surge_pct                 40000 non-null float64\n",
      "trips_in_first_30_days    40000 non-null int64\n",
      "luxury_car_user           40000 non-null bool\n",
      "weekday_pct               40000 non-null float64\n",
      "dtypes: bool(1), float64(6), int64(1), object(4)\n",
      "memory usage: 3.4+ MB\n"
     ]
    }
   ],
   "source": [
    "churn_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 12 columns):\n",
      "avg_dist                  10000 non-null float64\n",
      "avg_rating_by_driver      9961 non-null float64\n",
      "avg_rating_of_driver      8406 non-null float64\n",
      "avg_surge                 10000 non-null float64\n",
      "city                      10000 non-null object\n",
      "last_trip_date            10000 non-null object\n",
      "phone                     9923 non-null object\n",
      "signup_date               10000 non-null object\n",
      "surge_pct                 10000 non-null float64\n",
      "trips_in_first_30_days    10000 non-null int64\n",
      "luxury_car_user           10000 non-null bool\n",
      "weekday_pct               10000 non-null float64\n",
      "dtypes: bool(1), float64(6), int64(1), object(4)\n",
      "memory usage: 869.2+ KB\n"
     ]
    }
   ],
   "source": [
    "churn_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipline Custom Transfomer Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_attributes     = ['luxury_car_user']\n",
    "\n",
    "datetime_attributes    = [\"last_trip_date\", \"signup_date\"]\n",
    "\n",
    "categorical_attributes = [\"city\", \"phone\"]\n",
    "\n",
    "numerical_attributes   = [\"avg_dist\", \n",
    "                          \"avg_rating_by_driver\", \n",
    "                          \"avg_rating_of_driver\", \n",
    "                          \"avg_surge\", \n",
    "                          \"surge_pct\", \n",
    "                          \"trips_in_first_30_days\", \n",
    "                          \"weekday_pct\"]\n",
    "\n",
    "\n",
    "numerical_indices = churn_train.drop(datetime_attributes + categorical_attributes + boolean_attributes, axis=1).columns\n",
    "bool_indices = churn_train.drop(datetime_attributes + categorical_attributes + numerical_attributes, axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Selector Class\n",
    "this class will take in a dataframe X and return only the selected attributes of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, selected_attributes):\n",
    "        self.selected_attributes = selected_attributes\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        return X[self.selected_attributes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Class\n",
    "this class will take in a dataframe X and and return X after selectively adding in newly engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_weak_user=True, add_power_user=True, add_extreme_dist=True):\n",
    "        self.add_power_user = add_power_user\n",
    "        self.add_weak_user = add_weak_user\n",
    "        self.add_extreme_dist = add_extreme_dist\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        if self.add_weak_user:\n",
    "            weak_user = (X.loc[:, 'trips_in_first_30_days'] <= 2).astype(int)\n",
    "            X = pd.concat([X, weak_user.rename('weak_user')], axis=1)\n",
    "            \n",
    "        if self.add_power_user:\n",
    "            power_user = (X.loc[:, 'trips_in_first_30_days'] >= 5).astype(int)\n",
    "            X = pd.concat([X, power_user.rename('power_user')], axis=1)\n",
    "            \n",
    "        if self.add_extreme_dist:\n",
    "            bins = [0,1.8,10,100]\n",
    "            avg_distance_split = pd.cut(X['avg_dist'], bins=bins, labels=False)\n",
    "            X['extreme_dist'] = (avg_distance_split != 1).astype(int)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer Converter Class\n",
    "this class  will take in a dataframe X and return X with imputed values to missing data points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imputerConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_indices):\n",
    "        self.attribute_indices = attribute_indices\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        imputer = Imputer(strategy='median')\n",
    "        imputer.fit(X)\n",
    "        X = imputer.transform(X)\n",
    "        return pd.DataFrame(X, columns=self.attribute_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Converter Class\n",
    "this class will take in a dataframe and return a dataframe after converting tye types of certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class typeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, convert_to_int=True, convert_to_datetime=False):\n",
    "        self.convert_to_int = convert_to_int\n",
    "        self.convert_to_datetime = convert_to_datetime\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        if self.convert_to_int:\n",
    "            return X[:].astype(int)\n",
    "        elif self.convert_to_datetime:\n",
    "            return X.iloc[:, :].apply(pd.to_datetime, errors='coerce')\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Adder Class\n",
    "this class will take in a dataframe and return the dataframe after adding the target output column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class outputAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        end_date = pd.to_datetime('2014-07-01')\n",
    "        delta = pd.Timedelta('30 days')\n",
    "        \n",
    "        X['churn'] = ((end_date - X['last_trip_date']) >= delta).astype(int)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Dummies Class\n",
    "this class will take in a dataframe and return a dummified dataframe of all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class getDummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dummy_cols = []):\n",
    "        self.dummy_cols = dummy_cols\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        return pd.get_dummies(data=X, columns=self.dummy_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Dropper Class\n",
    "this class will take in a dataframe and return the dataframe after dropping a few selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class featureDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_list = []):\n",
    "        self.drop_list = drop_list\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        #assert isinstance(X, pd.DataFrame)\n",
    "        X.drop(self.drop_list, axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_pipe = Pipeline([\n",
    "                            ('selector', dataFrameSelector(datetime_attributes)),\n",
    "                            ('type converter', typeConverter(convert_to_int=False, convert_to_datetime=True)),\n",
    "                            ('feature dropper', featureDropper(datetime_attributes))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_pipe = Pipeline([\n",
    "                            ('selector', dataFrameSelector(numerical_attributes)),\n",
    "                            ('imputor', imputerConverter(numerical_indices)),\n",
    "                            ('feature engineer', featureEngineering()),\n",
    "                            ('scale', StandardScaler())\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_pipe = Pipeline([\n",
    "                        ('selector', dataFrameSelector(boolean_attributes)),\n",
    "                        ('imputor', imputerConverter(bool_indices)),\n",
    "                        ('type converter', typeConverter(convert_to_int=True, convert_to_datetime=False))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_pipe = Pipeline([\n",
    "                                ('selector', dataFrameSelector(categorical_attributes)),\n",
    "                                ('dummify', getDummies(categorical_attributes))\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_pipe = FeatureUnion(transformer_list=[\n",
    "                            ('num pipe', numerical_pipe),\n",
    "                            ('bool pipe', bool_pipe),\n",
    "                            ('date pipe', datetime_pipe),\n",
    "                            ('cat pipe', categorical_pipe)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 14)\n",
      "(10000, 14)\n"
     ]
    }
   ],
   "source": [
    "churn_train_X = pre_process_pipe.fit_transform(churn_train)\n",
    "churn_test_X = pre_process_pipe.fit_transform(churn_test)\n",
    "print(churn_train_X.shape)\n",
    "print(churn_test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Targets for Test/Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOutput(data_frame, output_col_name, return_type_numpy=True):\n",
    "    end_date = pd.to_datetime('2014-07-01')\n",
    "    delta = pd.Timedelta('30 days')\n",
    "    \n",
    "    data_frame['signup_date'] = pd.to_datetime(data_frame['signup_date'])\n",
    "    data_frame['last_trip_date'] = pd.to_datetime(data_frame['last_trip_date'])\n",
    "    data_frame[output_col_name] = ((end_date - data_frame['last_trip_date']) >= delta).astype(int)\n",
    "    \n",
    "    if return_type_numpy:\n",
    "        return data_frame[output_col_name].values\n",
    "    else:\n",
    "        return data_frame[output_col_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "churn_train_y = createOutput(churn_train, 'churn', return_type_numpy=True)\n",
    "churn_test_y = createOutput(churn_test, 'churn', return_type_numpy=True)\n",
    "print(churn_train_y.shape, churn_test_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight='balanced', dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=42,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.8, 0.9, 0.99, 1.01, 1.1]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C':[0.8, 0.9, 0.99, 1.01, 1.1]}\n",
    "logistic_clf = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "logistic_model = GridSearchCV(logistic_clf, param_grid, cv=10)\n",
    "\n",
    "logistic_model.fit(churn_train_X, churn_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.9}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_train_y_predict = logistic_model.predict(churn_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10015,  4620],\n",
       "       [ 6612, 18753]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(churn_train_y, churn_train_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7192"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(churn_train_y, churn_train_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7393258426966293"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(churn_train_y, churn_train_y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8023360287511231"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(churn_train_y, churn_train_y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=20, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=2,\n",
       "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=1000, n_jobs=-1, oob_score=False, random_state=42,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [1, 2]}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring=None, verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {   \n",
    "                'max_features': [1, 2]\n",
    "             }\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "                                    random_state=42,\n",
    "                                    n_jobs=-1,\n",
    "                                    class_weight='balanced',\n",
    "                                    min_samples_leaf=2,\n",
    "                                    bootstrap=True,\n",
    "                                    min_samples_split=10,\n",
    "                                    max_depth = 20,\n",
    "                                    n_estimators = 1000\n",
    "                                )\n",
    "\n",
    "rf_model = GridSearchCV(rf_clf, param_grid, cv=10)\n",
    "rf_model.fit(churn_train_X, churn_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 2}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_rf = rf_model.predict(churn_train_X)\n",
    "y_train_rf = createOutput(churn_train, 'churn', return_type_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrx = confusion_matrix(y_train_rf, y_train_predict_rf)\n",
    "accuracy_scr = accuracy_score(y_train_rf, y_train_predict_rf)\n",
    "recall_scr = recall_score(y_train_rf, y_train_predict_rf)\n",
    "precision_scr = precision_score(y_train_rf, y_train_predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12587,  2048],\n",
       "       [ 2846, 22519]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.87765, Recall:0.8877981470530258, Precision:0.9166361379085766\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:{}, Recall:{}, Precision:{}\".format(accuracy_scr, recall_scr, precision_scr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_rf = rf_model.predict(churn_test_X)\n",
    "y_test_rf = createOutput(churn_test, 'churn', return_type_numpy=True)\n",
    "\n",
    "confusion_matrx = confusion_matrix(y_test_rf, y_test_predict_rf)\n",
    "accuracy_scr = accuracy_score(y_test_rf, y_test_predict_rf)\n",
    "recall_scr = recall_score(y_test_rf, y_test_predict_rf)\n",
    "precision_scr = precision_score(y_test_rf, y_test_predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2618, 1057],\n",
       "       [1162, 5163]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7781, Recall:0.8162845849802371, Precision:0.830064308681672\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:{}, Recall:{}, Precision:{}\".format(accuracy_scr, recall_scr, precision_scr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
